{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOny8mNSrGYidEZVM/LKlJb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 1: Instala√ß√£o Completa (CORRIGIDA)\n",
        "\n",
        "print(\"‚è≥ Instalando debgpt...\")\n",
        "!pip3 install debgpt &> /dev/null\n",
        "print(\"‚úÖ debgpt instalado.\")\n",
        "\n",
        "print(\"\\n‚è≥ Instalando o servidor Ollama...\")\n",
        "# Usamos o script oficial de instala√ß√£o do Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh &> /dev/null\n",
        "print(\"‚úÖ Servidor Ollama instalado.\")\n",
        "\n",
        "# Verifique a GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztOPOJJLb5eD",
        "outputId": "1416defa-1001-46f3-fc8b-c1f9d2b97926"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Instalando debgpt...\n",
            "‚úÖ debgpt instalado.\n",
            "\n",
            "‚è≥ Instalando o servidor Ollama...\n",
            "‚úÖ Servidor Ollama instalado.\n",
            "Fri Aug 15 02:22:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             44W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ollama.log\n",
        "!cat ollama.err.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6F8IqcTjNid",
        "outputId": "2e3c1576-b5f1-4c18-9c9f-b460050a333d"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GIN] 2025/08/15 - 05:07:54 | 200 |      49.393¬µs |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2025/08/15 - 05:07:54 | 200 |   97.528292ms |       127.0.0.1 | POST     \"/api/show\"\n",
            "[GIN] 2025/08/15 - 05:11:07 | 200 |         3m13s |       127.0.0.1 | POST     \"/api/generate\"\n",
            "[GIN] 2025/08/15 - 05:12:46 | 200 | 14.320229528s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:19:20 | 200 |      37.335¬µs |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2025/08/15 - 05:19:20 | 200 |   84.329961ms |       127.0.0.1 | POST     \"/api/show\"\n",
            "[GIN] 2025/08/15 - 05:19:34 | 200 | 13.557195116s |       127.0.0.1 | POST     \"/api/generate\"\n",
            "[GIN] 2025/08/15 - 05:21:31 | 200 | 39.868737773s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:22:35 | 200 | 42.040310867s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:23:53 | 200 |  37.12404442s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:24:54 | 200 | 29.810735191s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:26:08 | 200 |  35.73964009s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:29:41 | 200 | 51.943487993s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "[GIN] 2025/08/15 - 05:30:14 | 200 |      93.475¬µs |       127.0.0.1 | GET      \"/\"\n",
            "[GIN] 2025/08/15 - 05:36:33 | 200 |      89.344¬µs |       127.0.0.1 | GET      \"/\"\n",
            "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n",
            "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000time=2025-08-15T05:07:55.152Z level=INFO source=server.go:135 msg=\"system memory\" total=\"83.5 GiB\" free=\"80.9 GiB\" free_swap=\"0 B\"\n",
            "time=2025-08-15T05:07:55.153Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=80 layers.split=\"\" memory.available=\"[39.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"39.9 GiB\" memory.required.partial=\"39.1 GiB\" memory.required.kv=\"1.2 GiB\" memory.required.allocations=\"[39.1 GiB]\" memory.weights.total=\"36.7 GiB\" memory.weights.repeating=\"35.9 GiB\" memory.weights.nonrepeating=\"822.0 MiB\" memory.graph.full=\"584.0 MiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_0:  561 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 37.22 GiB (4.53 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 1\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 70.55 B\n",
            "print_info: general.name     = Meta-Llama-3-70B-Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "time=2025-08-15T05:07:55.535Z level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d --ctx-size 4096 --batch-size 512 --n-gpu-layers 80 --threads 6 --parallel 1 --port 46143\"\n",
            "time=2025-08-15T05:07:55.536Z level=INFO source=sched.go:481 msg=\"loaded runners\" count=1\n",
            "time=2025-08-15T05:07:55.536Z level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-08-15T05:07:55.536Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
            "time=2025-08-15T05:07:55.549Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
            "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
            "time=2025-08-15T05:07:55.629Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
            "time=2025-08-15T05:07:55.632Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:46143\"\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40082 MiB free\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "time=2025-08-15T05:07:55.787Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_0:  561 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 37.22 GiB (4.53 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 8192\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 80\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 28672\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 8192\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 70B\n",
            "print_info: model params     = 70.55 B\n",
            "print_info: general.name     = Meta-Llama-3-70B-Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 80 repeating layers to GPU\n",
            "load_tensors: offloaded 80/81 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 36725.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size = 38110.61 MiB\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.52 MiB\n",
            "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB\n",
            "llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =  1088.45 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_context: graph nodes  = 2726\n",
            "llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)\n",
            "time=2025-08-15T05:11:07.971Z level=INFO source=server.go:637 msg=\"llama runner started in 192.43 seconds\"\n",
            "time=2025-08-15T05:19:21.370Z level=INFO source=server.go:135 msg=\"system memory\" total=\"83.5 GiB\" free=\"81.4 GiB\" free_swap=\"0 B\"\n",
            "time=2025-08-15T05:19:21.371Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=80 layers.split=\"\" memory.available=\"[39.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"39.9 GiB\" memory.required.partial=\"39.1 GiB\" memory.required.kv=\"1.2 GiB\" memory.required.allocations=\"[39.1 GiB]\" memory.weights.total=\"36.7 GiB\" memory.weights.repeating=\"35.9 GiB\" memory.weights.nonrepeating=\"822.0 MiB\" memory.graph.full=\"584.0 MiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_0:  561 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 37.22 GiB (4.53 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 1\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 70.55 B\n",
            "print_info: general.name     = Meta-Llama-3-70B-Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "time=2025-08-15T05:19:21.748Z level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d --ctx-size 4096 --batch-size 512 --n-gpu-layers 80 --threads 6 --parallel 1 --port 34063\"\n",
            "time=2025-08-15T05:19:21.748Z level=INFO source=sched.go:481 msg=\"loaded runners\" count=1\n",
            "time=2025-08-15T05:19:21.748Z level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-08-15T05:19:21.748Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
            "time=2025-08-15T05:19:21.762Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
            "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
            "time=2025-08-15T05:19:22.066Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
            "time=2025-08-15T05:19:22.086Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:34063\"\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40082 MiB free\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /root/.ollama/models/blobs/sha256-0bd51f8f0c975ce910ed067dcb962a9af05b77bafcdc595ef02178387f10e51d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-08-15T05:19:22.251Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_0:  561 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 37.22 GiB (4.53 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 8192\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 80\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 28672\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 8192\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 70B\n",
            "print_info: model params     = 70.55 B\n",
            "print_info: general.name     = Meta-Llama-3-70B-Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 80 repeating layers to GPU\n",
            "load_tensors: offloaded 80/81 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 36725.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size = 38110.61 MiB\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.52 MiB\n",
            "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB\n",
            "llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =  1088.45 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_context: graph nodes  = 2726\n",
            "llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)\n",
            "time=2025-08-15T05:19:34.544Z level=INFO source=server.go:637 msg=\"llama runner started in 12.80 seconds\"\n",
            "time=2025-08-15T05:20:51.826Z level=WARN source=runner.go:128 msg=\"truncating input prompt\" limit=4096 prompt=8864 keep=25 new=4096\n",
            "time=2025-08-15T05:28:49.564Z level=WARN source=runner.go:128 msg=\"truncating input prompt\" limit=4096 prompt=8885 keep=25 new=4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula √önica: Ollama + Ngrok + Configura√ß√£o debgpt\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- CONFIGURA√á√ÉO ---\n",
        "SSH_PUBLIC_KEY = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC+71DgrVmYbomnQ/hGQPmYAO3Whq4CDut7c7Tfb6bLgbko4nkElutLCxjSFSEp3T+BhzV2J7LLxhmabbBPOlq6Gk/3Hm3DNbJyN06TFY1vacmaOGNVDdRbl6DCHkPiw9k4U/LUE+ihQSRH2ZOcCUzadNO1d1VEOZp8MtAinmZNfzzBObBd9jjLW/iUQVqYb7Em3XpC4/pGGAqSFSMBiKLMGOVI+PDh7KVQchd9xSSw6owKofpNMz3J5kC7R30izgQk5iJxwwlH535r26w8CWeOAveVs32h3bbxRhrEhHoAx5FBAMZgf6lI4FDijBSbqCldTXEL91vYq29Hj1r6M5IWEF34ZAa3Ozb3XlVjW8x2Joe5LF+En+9MFea2U/vaBlt2YBHT8HD/b6Hj0Fa85g5tL3ZLKuTPsnz1sYRB9sQOGWdZDaLSx7vPJh26NbjqLI9LRcX4JrF/hqmMHZlxiu/cQwB+luaUCvLnmkE2yQt0hNELt0ZYQ09gX4j7EGB7P/M= danilomacedo1@gmail.com\"    # Cole sua chave p√∫blica SSH\n",
        "NGROK_AUTH_TOKEN = \"2JXxvWyFQl4xwMpa3OYoxNAZa1a_7nj1Wq7YKkCTyTira54cS\"  # Cole seu token Ngrok\n",
        "DEBGPT_CONFIG_PATH = \"/root/.debgpt/config.toml\"\n",
        "\n",
        "# --- 1. Instalar depend√™ncias ---\n",
        "print(\"‚è≥ Instalando OpenSSH e pyngrok...\")\n",
        "os.system(\"apt-get update -qq > /dev/null\")\n",
        "os.system(\"apt-get install -qq -y openssh-server net-tools &> /dev/null\")\n",
        "os.system(\"pip install pyngrok --quiet\")\n",
        "print(\"‚úÖ Depend√™ncias instaladas.\")\n",
        "\n",
        "# --- 2. Configurar SSH ---\n",
        "print(\"\\n‚öôÔ∏è Configurando SSH...\")\n",
        "os.system(\"sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config\")\n",
        "os.system(\"sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config\")\n",
        "os.system(\"echo 'ChallengeResponseAuthentication no' >> /etc/ssh/sshd_config\")\n",
        "\n",
        "ssh_dir = Path(\"/root/.ssh\")\n",
        "ssh_dir.mkdir(exist_ok=True, mode=0o700)\n",
        "authorized_keys_path = ssh_dir / \"authorized_keys\"\n",
        "authorized_keys_path.write_text(SSH_PUBLIC_KEY)\n",
        "authorized_keys_path.chmod(0o600)\n",
        "\n",
        "os.system(\"service ssh start > /dev/null\")\n",
        "print(\"‚úÖ SSH configurado e servi√ßo iniciado.\")\n",
        "\n",
        "# --- 3. Configurar Ngrok ---\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# --- 4. Criar t√∫neis ---\n",
        "# SSH\n",
        "ssh_tunnel = ngrok.connect(22, \"tcp\")\n",
        "ssh_host, ssh_port = ssh_tunnel.public_url.replace(\"tcp://\", \"\").split(\":\")\n",
        "\n",
        "# Ollama (porta padr√£o 11434)\n",
        "ollama_tunnel = ngrok.connect(11434, \"tcp\")\n",
        "ollama_host, ollama_port = ollama_tunnel.public_url.replace(\"tcp://\", \"\").split(\":\")\n",
        "\n",
        "print(f\"\\n‚úÖ T√∫neis criados:\")\n",
        "print(f\"SSH -> ssh root@{ssh_host} -p {ssh_port}\")\n",
        "print(f\"Ollama -> {ollama_host}:{ollama_port}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ü§ñ 5. INICIALIZA√á√ÉO DO OLLAMA E DO MODELO (VERS√ÉO CORRIGIDA)\n",
        "# ==============================================================================\n",
        "\n",
        "async def run_ollama_serve():\n",
        "    \"\"\"Inicia o servidor Ollama e aguarda ele ficar pronto.\"\"\"\n",
        "    print(\"\\n‚è≥ Iniciando servidor Ollama em segundo plano...\")\n",
        "    ollama_process = subprocess.Popen(\n",
        "        ['nohup', 'ollama', 'serve'],\n",
        "        stdout=open('ollama.log', 'w'),\n",
        "        stderr=open('ollama.err.log', 'w'),\n",
        "        preexec_fn=os.setpgrp\n",
        "    )\n",
        "    # Damos um tempo generoso para o servidor inicializar completamente\n",
        "    print(\"... Aguardando o servidor Ollama ficar online (at√© 30s)...\")\n",
        "    await asyncio.sleep(15) # Aumentado para garantir que a porta esteja livre\n",
        "\n",
        "    # Verificamos se o processo ainda est√° rodando\n",
        "    if ollama_process.poll() is None:\n",
        "        print(\"‚úÖ Servidor Ollama iniciado com sucesso.\")\n",
        "    else:\n",
        "        print(\"‚ùå Falha ao iniciar Ollama. Verifique 'ollama.log' e 'ollama.err.log'.\")\n",
        "        # Se o servidor n√£o iniciar, n√£o adianta continuar\n",
        "        return None\n",
        "    return ollama_process\n",
        "\n",
        "async def setup_model(model_name):\n",
        "    \"\"\"Baixa e pr√©-carrega um modelo usando um servidor Ollama j√° em execu√ß√£o.\"\"\"\n",
        "    print(f\"\\n‚è≥ Baixando e pr√©-carregando o modelo '{model_name}'...\")\n",
        "    print(\"(Isso pode levar alguns minutos dependendo do tamanho do modelo)\")\n",
        "    try:\n",
        "        # Usamos 'ollama pull' primeiro para um download limpo, depois 'run' para carregar.\n",
        "        # Isso evita que 'run' tente iniciar um segundo servidor.\n",
        "        subprocess.run(\n",
        "            f'ollama pull {model_name}',\n",
        "            shell=True, check=True, capture_output=True, text=True, timeout=1800 # 30 min timeout\n",
        "        )\n",
        "        print(f\"‚úÖ Download de '{model_name}' conclu√≠do.\")\n",
        "\n",
        "        print(f\"‚è≥ Carregando '{model_name}' na VRAM da GPU...\")\n",
        "        # Apenas carregamos o modelo, ele j√° sabe que o servidor est√° rodando\n",
        "        subprocess.run(\n",
        "            f'ollama run {model_name} \"Responda apenas com OK\"',\n",
        "            shell=True, check=True, capture_output=True, text=True, timeout=600 # 10 min timeout\n",
        "        )\n",
        "        print(f\"‚úÖ Modelo '{model_name}' pronto para uso!\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erro ao baixar/carregar modelo: {e.stdout} {e.stderr}\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"‚ùå Timeout ao interagir com o modelo. Tente novamente ou verifique os logs.\")\n",
        "\n",
        "# --- Nova Ordem de Execu√ß√£o ---\n",
        "# 1. Iniciar o servidor PRIMEIRO\n",
        "ollama_server_process = await run_ollama_serve()\n",
        "\n",
        "# 2. Se o servidor iniciou com sucesso, carregar o modelo\n",
        "if ollama_server_process:\n",
        "    MODEL_TO_LOAD = \"llama3:70b\"\n",
        "    await setup_model(MODEL_TO_LOAD)\n",
        "else:\n",
        "    print(\"‚ùå N√£o foi poss√≠vel continuar, pois o servidor Ollama falhou ao iniciar.\")\n",
        "\n",
        "# --- 6. Atualizar config.toml do debgpt ---\n",
        "config_content = f\"\"\"\n",
        "[default]\n",
        "frontend = \"ollama\"\n",
        "mapreduce_parallelism = 32\n",
        "mapreduce_chunksize = 200000\n",
        "\n",
        "[ollama]\n",
        "base_url = \"http://{ollama_host}:{ollama_port}\"\n",
        "model = \"mixtral\"\n",
        "google_api_key = \"AIzaSyBAgc0_j8faHRGXfsyZCAdJowSAmwzN4bM\"\n",
        "google_cx_id = \"24a9707e1e0134393\"\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(os.path.dirname(DEBGPT_CONFIG_PATH), exist_ok=True)\n",
        "with open(DEBGPT_CONFIG_PATH, \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"\\n‚úÖ Configura√ß√£o debgpt atualizada com host/port do Ollama.\")\n",
        "\n",
        "# --- 7. Instru√ß√µes finais ---\n",
        "print(\"\\n‚ÑπÔ∏è Use os seguintes comandos no seu PC:\")\n",
        "print(f\"SSH: ssh root@{ssh_host} -p {ssh_port}\")\n",
        "print(f\"Port-forward para Ollama: ssh -L 11434:{ollama_host}:{ollama_port} root@{ssh_host} -p {ssh_port}\")\n",
        "print(\"Depois abra no navegador: http://127.0.0.1:11434\")\n",
        "\n",
        "safety_settings = {\n",
        "    \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"BLOCK_NONE\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qnNAvuPhEsU",
        "outputId": "9c6aa4c5-a844-4454-abab-33aaffa7f775"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Instalando OpenSSH e pyngrok...\n",
            "‚úÖ Depend√™ncias instaladas.\n",
            "\n",
            "‚öôÔ∏è Configurando SSH...\n",
            "‚úÖ SSH configurado e servi√ßo iniciado.\n",
            "\n",
            "‚úÖ T√∫neis criados:\n",
            "SSH -> ssh root@8.tcp.ngrok.io -p 17750\n",
            "Ollama -> 4.tcp.ngrok.io:10826\n",
            "\n",
            "‚è≥ Iniciando servidor Ollama em segundo plano...\n",
            "... Aguardando o servidor Ollama ficar online (at√© 30s)...\n",
            "‚ùå Falha ao iniciar Ollama. Verifique 'ollama.log' e 'ollama.err.log'.\n",
            "‚ùå N√£o foi poss√≠vel continuar, pois o servidor Ollama falhou ao iniciar.\n",
            "\n",
            "‚úÖ Configura√ß√£o debgpt atualizada com host/port do Ollama.\n",
            "\n",
            "‚ÑπÔ∏è Use os seguintes comandos no seu PC:\n",
            "SSH: ssh root@8.tcp.ngrok.io -p 17750\n",
            "Port-forward para Ollama: ssh -L 11434:4.tcp.ngrok.io:10826 root@8.tcp.ngrok.io -p 17750\n",
            "Depois abra no navegador: http://127.0.0.1:11434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "VYpYOqpZAmo5"
      },
      "execution_count": 153,
      "outputs": []
    }
  ]
}